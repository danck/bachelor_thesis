\documentclass[draft=false
              ,paper=a4
              ,twoside=false
              ,fontsize=11pt
              ,headsepline
              ,BCOR10mm
              ,DIV11
              ]{scrbook}
\usepackage{graphicx}
\usepackage[ngerman,english]{babel}
%% see http://www.tex.ac.uk/cgi-bin/texfaq2html?label=uselmfonts
\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
\usepackage[latin1]{inputenc}
\usepackage{libertine}
\usepackage{pifont}
\usepackage{microtype}
\usepackage{textcomp}
\usepackage[german,refpage]{nomencl}
\usepackage{setspace}
\usepackage{makeidx}
\usepackage{listings}
\usepackage{natbib}
\usepackage[ngerman,colorlinks=true]{hyperref}
\usepackage{soul}
\usepackage{hawstyle}
\usepackage{lipsum} %% for sample text

\usepackage{svg}
\usepackage{amsmath}
\usepackage{xcolor}

\setsvg{inkscape={"C:/Program Files/Inkscape/inkscape.exe"= -z -C}}

%% define some colors
\colorlet{BackgroundColor}{gray!20}
\colorlet{KeywordColor}{blue}
\colorlet{CommentColor}{black!60}
%% for tables
\colorlet{HeadColor}{gray!60}
\colorlet{Color1}{blue!10}
\colorlet{Color2}{white}

%% configure colors
\HAWifprinter{
  \colorlet{BackgroundColor}{gray!20}
  \colorlet{KeywordColor}{black}
  \colorlet{CommentColor}{gray}
  % for tables
  \colorlet{HeadColor}{gray!60}
  \colorlet{Color1}{gray!40}
  \colorlet{Color2}{white}
}{}
\lstset{%
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{KeywordColor}\bfseries,
  identifierstyle=\color{black},
  commentstyle=\color{CommentColor},
  backgroundcolor=\color{BackgroundColor},
  captionpos=b,
  fontadjust=true
}
\lstset{escapeinside={(*@}{@*)}, % used to enter latex code inside listings
        morekeywords={uint32_t, int32_t}
}
\ifpdfoutput{
  \hypersetup{bookmarksopen=false,bookmarksnumbered,linktocpage}
}{}

%% more fancy C++
\DeclareRobustCommand{\cxx}{C\raisebox{0.25ex}{{\scriptsize +\kern-0.25ex +}}}

\clubpenalty=10000
\widowpenalty=10000
\displaywidowpenalty=10000

% unknown hyphenations
\hyphenation{
}

%% recalculate text area
\typearea[current]{last}

\makeindex
\makenomenclature

\begin{document}
\selectlanguage{ngerman}

%%%%%
%% customize (see readme.pdf for supported values)
\HAWThesisProperties{Author={Daniel Kirchner}
                    ,Title={Skalierbare Datenanalyse mit Apache Spark}
										,SubTitle={Evaluation von Anwendungsfällen aus Klimaforschung und Text-Mining}
                    ,EnglishTitle={Scalable Data Analysis with Apache Spark}
                    ,ThesisType={Bachelorarbeit}
                    ,ExaminationType={Bachelorprüfung}
                    ,DegreeProgramme={Bachelor of Science Angewandte Informatik}
                    ,ThesisExperts={Prof. Dr. Kahlbrandt \and Prof. Dr. Zweitprüfer}
                    ,ReleaseDate={1. Januar 2345}
                  }

%% title
\frontmatter

%% output title page
\maketitle

\onehalfspacing

%% add abstract pages
%% note: this is one command on multiple lines
\HAWAbstractPage
%% German abstract
{Schlüsselwort 1, Schlüsselwort 2}%
{Dieses Dokument \ldots}
%% English abstract
{keyword 1, keyword 2}%
{This document \ldots}

\newpage
\singlespacing

\tableofcontents

\newpage
%% enable if these lists should be shown on their own page
%%\listoftables
%%\listoffigures
\lstlistoflistings

%% main
\mainmatter
\onehalfspacing
%% write to the log/stdout
\typeout{===== File: chapter 1}
%% include chapter file (chapter1.tex)
%%\include{chapter1}

%%%%
%% add some text to generate a sample document
\chapter{Einf\"uhrung}

\section{Motivation}

Die Entwicklung und Verbesserung von Frameworks zur Verarbeitung großer Datenmengen ist zur Zeit hochaktuell und sehr im Fokus von Medien und Unternehmen [VERWEIS]. Verschiedene Programme und Paradigmen konkurrieren um die schnellste, bequemste und stabilste Art großen Datenmengen einen geschäftsfördenden Nutzen abzuringen.
\break

Unter dem Begriff "`große Datenmengen"' oder "`Big Data"' werden solche Datenmengen zusammengefasst, die die Kriterien Volume, Velocity, Variety [VERWEIS, Doug Laney] erfüllen oder "`Datenmengen, die nicht mehr unter Auflage bestimmter SLAs auf einzelnen Maschinen verarbeitet werden können"' [VERWEIS, Hadoop/Yarn Entwickler].

Als Unternehmen, das früh mit solchen Datenmengen konfrontiert war implementierte Google das Map-Reduce Paradigma [VERWEIS] als Framework zur Ausnutzung vieler kostengünstiger Rechner um Webseiten einzustufen und für andere Aufgaben [VERWEIS]. 

In Folge der Veröffentlichung ihrer Idee im Jahr 2005 [VERWEIS] wurde Map-Reduce in Form der OpenSource Implementation Hadoop (gemeinsam mit einer Implementation des Google File Systems GFS, u.a.) [VERWEIS] zum de-facto Standard für Big-Data-Analyseaufgaben [VERWEIS?].
\break

Reines Map-Reduce (nach Art von Hadoop) als Programmierparadigma zur Verarbeitung großer Datenmengen zeigt jedoch in vielen Anwendungsfällen Schwächen:
\begin{itemize}
	\item Daten, die in hoher Frequenz entstehen und schnell verarbeitet werden sollen erfordern häufiges Neustarten von Map-Reduce-Jobs.
	\item Algorithmen die während ihrer Ausführung iterativ Zwischenergebnisse erzeugen und auf vorherige angewiesen sind (typischerweise Maschinenlernalgorithmen) können nur durch persistentes Speichern der Daten und wiederholtes Auslesen zwischen allen Iterationsschritten implementiert werden.
	\item Generell erfolgen Anfragen an ein solches System immer in Form von kleinen Programmen. Dieses Verfahren ist offensichtlich nicht so deklarativ und leicht erlernbar wie beispielsweise SQL-Anfragen an klassische Datenbanken.
\end{itemize}

In der Folge entstanden viele Ansätze dieses Paradigma zu ersetzen, zu ergänzen oder durch übergeordnete Ebenen und High-Level-APIs zu vereinfachen.

\begin{itemize}
	\item {[}VERWEIS: A survey of large scale...{]} oder Aufzählung.
\end{itemize}

Eine der Alternativen zu der Map-Reduce-Komponente in Hadoop die "`general engine for large-scale data processing"' Apache Spark.

Ein Indiz für das steigende Interesse an diesem Produkt liefert unter anderem ein Vergleich des Interesses an Hadoop und Spark auf Google:
\break

\includegraphics[scale=0.4]{bilder/trends_spark_vs_hadoop.PNG}

\section{Kontextabgrenzung}
Das Ziel dieser Arbeit ist es einen Einblick in die grundlegenden Konzepte und Anwendungsmöglichkeiten von Apache Spark zu vermitteln.

Für ein tieferes Verständnis werden zwei Anwendungsfälle untersucht und deren Lösung detailiert dokumentiert und bewertet.
\break

Nur am Rande wird betrachtet:
\begin{itemize}
	\item Vergleich mit ähnlichen Produkten
	\item Empirische Messung des Skalierungsverhaltens
	\item Konkrete Hinweise zu Installation und Nutzung
\end{itemize}

\chapter{Vorstellung von Apache Spark}

\section{Überblick}

\section{Kernkonzepte}
\textcolor{gray}{--- Warum ist Spark so schnell (und wo vielleicht nicht)? ---}
\subsection{Resilient Distributed Datasets}
\subsection{Lineage}
\subsection{DAG Scheduler}

\section{Standardbibliotheken}
\textcolor{gray}{--- Warum ist Spark so einfach (und wo vielleicht nicht)? ---}
\subsection{Spark SQL}
\subsection{MLlib}
\subsection{Streaming}
\subsection{GraphX}

\section{Entwicklergemeinschaft}

\section{Verwandte Produkte}
\textcolor{gray}{--- Ergänzende oder konkurrierende Produkte ---}
\subsection{YARN}
\subsection{Mesos}
\subsection{Flink}


\chapter{Untersuchung von Anwendungsfällen}
Im Folgenden wird Apache Spark im Rahmen zweier grundsätzlich verschiedener Anwendungsfälle betrachtet.
\break

Beispiel 1: Eine typische Anwendung mit verteilten lokalem Storage (HDFS) und Spark als "`Client"' eines bestehenden Yarn Clustermanagers. \textcolor{gray}{--- Commodity Hardware (Rasperry Pi Cluster). ---}
\break

Beispiel 2: Eine untypische Anwendung mit verteiltem entfernten Storage und dem Spark Standalone Clustermanager. \textcolor{gray}{--- HPC Hardware ("`Thunder"' des Hamburger KlimaCampus). ---}

\section{Identifikation von Hot Topics in der Spark Community}
\textcolor{gray}{--- Fusion von Tweets und Mailinglisten 
https://spark.apache.org/docs/1.3.0/mllib-feature-extraction.html
Implementation auf einem Raspberry Pi Cluster mit HDFS und Yarn Clustermanager ---}
\subsection{Beschreibung des Problems}

\subsection{Hardwarekontext und Performance-Basisdaten}
\textcolor{gray}{--- hier kommen die eingesetzten systeme, und relevante laufzeitmessungen (netzwerk, storage, cpu) hin ---}

%%\begin{figure}[htbp]
%%  \centering
  \includesvg[width=\paperwidth]{versuchsaufbau}
%%  \caption{svg image}
%%\end{figure}

\subsection{Architekturübersicht}
\textcolor{gray}{--- hier kommen Verteilungs- und Komponentendiagramm hin ---}
\subsection{Detailierte Lösungsbeschreibung}
\textcolor{gray}{--- hier kommen laufzeitdiagramme und codeschnipsel hin ---}
\subsection{Ergebnisse}
\textcolor{gray}{--- Tabellen und Diagramme Ergebnissen, evt. Skalierungsverhalten ---}
\textcolor{gray}{--- Bewertung ---}

\section{Evaluierung einer spark-basierten Implementation von CDOs auf einem HPC Cluster mit nicht-lokalem Storage}
\textcolor{gray}{--- Implementation ausgewählter CDOs (sehr wenige, möglicherweise nur 1-2) mit der Core-API von Spark. Testlauf auf einem HPC Cluster mit nicht-lokalem, allerdings per Infiniband angeschlossenen Storage.
Insbesondere Betrachtung des Skalierungsverhaltens und der "`Sinnhaftigkeit"'. ---}

\subsection{Beschreibung des Problems}
\textcolor{gray}{--- Erläuterung von CDOs (Climate Data Operators). ---}
\subsection{Hardwarekontext und Performance-Basisdaten}
\textcolor{gray}{--- hier kommen die eingesetzten systeme, und relevante laufzeitmessungen (netzwerk, storage, cpu) hin ---}
\subsection{Architekturübersicht}
\textcolor{gray}{--- hier kommen Verteilungs- und Komponentendiagramm hin ---}
\subsection{Detailierte Lösungsbeschreibung}
\textcolor{gray}{--- hier kommen laufzeitdiagramme und codeschnipsel hin ---}
\subsection{Ergebnisse}
\textcolor{gray}{--- Tabellen und Diagramme Ergebnissen, evt. Skalierungsverhalten ---}
\textcolor{gray}{--- Bewertung ---}

\chapter{Schlussbetrachtung}
\section{Kritische W\"urdigung der Ergebnisse}
\section{Ausblick und offene Punkte}

%%\lipsum

See also \cite{sample_bib}.
%%%%

%% appendix if used
%%\appendix
%%\typeout{===== File: appendix}
%%\include{appendix}

% bibliography and other stuff
\backmatter

\typeout{===== Section: literature}
%% read the documentation for customizing the style
\bibliographystyle{dinat}
\bibliography{sample}

\typeout{===== Section: nomenclature}
%% uncomment if a TOC entry is needed
%%\addcontentsline{toc}{chapter}{Glossar}
\renewcommand{\nomname}{Glossar}
\clearpage
\markboth{\nomname}{\nomname} %% see nomencl doc, page 9, section 4.1
\printnomenclature

%% index
\typeout{===== Section: index}
\printindex

\HAWasurency

\end{document}
