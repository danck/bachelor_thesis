\documentclass[draft=false
              ,paper=a4
              ,twoside=false
              ,fontsize=11pt
              ,headsepline
              ,BCOR10mm
              ,DIV11
              ]{scrbook}
\usepackage{graphicx}
\usepackage[ngerman,english]{babel}
%% see http://www.tex.ac.uk/cgi-bin/texfaq2html?label=uselmfonts
\usepackage[T1]{fontenc}
%\usepackage[utf8]{inputenc}
\usepackage[utf8]{inputenc}
\usepackage{libertine}
\usepackage{pifont}
\usepackage{microtype}
\usepackage{textcomp}
%\usepackage[german,refpage]{nomencl}
\usepackage{setspace}
\usepackage{makeidx}
\usepackage{listings}
%\usepackage{natbib}
\usepackage[ngerman,colorlinks=true]{hyperref}
\usepackage{soul}
\usepackage{hawstyle}
\usepackage{lipsum} %% for sample text


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[backend=bibtex,bibstyle=alphabetic,citestyle=alphabetic,]{biblatex}
\addbibresource{sample.bib}
\renewcommand{\bibfootnotewrapper}[1]{%
  \bibsentence\mkbibbrackets{#1}\addperiod}
%\bibliography{sample}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{fullpage} %% Wirklich?
\usepackage{tikz}
\usepackage{tikz-uml}
\usepackage[toc]{glossaries}
\usepackage{listings}
\def\dollar{\$}
\lstset{
        literate={my_dollar}{\$}1
}

\setsvg{inkscape={"C:/Program Files/Inkscape/inkscape.exe"= -z -C}}

\tikzstyle{RectObject}=[rectangle,fill=white,draw,line width=0.5mm]
\tikzstyle{line}=[draw]
\tikzstyle{arrow}=[draw, -latex] 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% define some colors
\colorlet{BackgroundColor}{gray!20}
\colorlet{KeywordColor}{blue}
\colorlet{CommentColor}{black!60}
%% for tables
\colorlet{HeadColor}{gray!60}
\colorlet{Color1}{blue!10}
\colorlet{Color2}{white}

%% configure colors
\HAWifprinter{
  \colorlet{BackgroundColor}{gray!20}
  \colorlet{KeywordColor}{black}
  \colorlet{CommentColor}{gray}
  % for tables
  \colorlet{HeadColor}{gray!60}
  \colorlet{Color1}{gray!40}
  \colorlet{Color2}{white}
}{}
\lstset{%
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=5pt,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{KeywordColor}\bfseries,
  identifierstyle=\color{black},
  commentstyle=\color{CommentColor},
  backgroundcolor=\color{BackgroundColor},
  captionpos=b,
  fontadjust=true
}
\lstset{escapeinside={(*@}{@*)}, % used to enter latex code inside listings
        morekeywords={uint32_t, int32_t}
}
\ifpdfoutput{
  \hypersetup{bookmarksopen=false,bookmarksnumbered,linktocpage}
}{}

%% more fancy C++
\DeclareRobustCommand{\cxx}{C\raisebox{0.25ex}{{\scriptsize +\kern-0.25ex +}}}

\clubpenalty=10000
\widowpenalty=10000
\displaywidowpenalty=10000

% unknown hyphenations
\hyphenation{
}

%% recalculate text area
\typearea[current]{last}

\makeindex
%\makenomenclature
\makeglossaries

\begin{document}
\selectlanguage{ngerman}

%%%%%
%% customize (see readme.pdf for supported values)
\HAWThesisProperties{Author={Daniel Kirchner}
                    ,Title={Skalierbare Datenanalyse mit Apache Spark}
										,SubTitle={}
                    ,EnglishTitle={Scalable Data Analysis with Apache Spark}
                    ,ThesisType={Bachelorarbeit}
                    ,ExaminationType={Bachelorprüfung}
                    ,DegreeProgramme={Bachelor of Science Angewandte Informatik}
                    ,ThesisExperts={Prof. Dr. Kahlbrandt \and Prof. Dr. Zukunft}
                    ,ReleaseDate={1. Januar 2345}
                  }

%% title
\frontmatter

%% output title page
\maketitle

\onehalfspacing

%% add abstract pages
%% note: this is one command on multiple lines
\HAWAbstractPage
%% German abstract
{Schlüsselwort 1, Schlüsselwort 2}%
{Dieses Dokument \ldots}
%% English abstract
{keyword 1, keyword 2}%
{This document \ldots}

\newpage
\singlespacing

\tableofcontents

\newpage
%% enable if these lists should be shown on their own page
\listoftables
\listoffigures
\lstlistoflistings

%% main
\mainmatter
\onehalfspacing
%% write to the log/stdout
\typeout{===== File: chapter 1}
%% include chapter file (chapter1.tex)
%%\include{chapter1}
\include{glossar}
%%%%
%% add some text to generate a sample document
\chapter{Einführung}

\section{Motivation}

Die Entwicklung TESTETSTETS22222222 \gls{repl} und Verbesserung von Frameworks zur Verarbeitung großer Datenmengen ist zur Zeit hochaktuell und sehr im Fokus von Medien und Unternehmen [VERWEIS]. Verschiedene Programme und Paradigmen konkurrieren um die schnellste, bequemste und stabilste Art großen Datenmengen einen geschäftsfördenden Nutzen abzuringen.\\

Mit dem Begriff "`große Datenmengen"' oder "`Big Data"' werden in dieser Arbeit solche Datenmengen zusammengefasst, die die Kriterien Volume, Velocity, Variety \footcite{Lan01} erfüllen oder "`Datenmengen, die nicht mehr unter Auflage bestimmter SLAs auf einzelnen Maschinen verarbeitet werden können"' (Vgl. \cite{Sam14}).\\

Als Unternehmen, das früh mit solchen Datenmengen konfrontiert war implementierte Google das Map-Reduce Paradigma \footcite{Dean04} als Framework zur Ausnutzung vieler kostengünstiger Rechner für verschiedene Aufgaben (u.a.  Indizierung von Webseiten und PageRank \footcite{page2001method}). \\

In Folge der Veröffentlichung dieser Idee im Jahr 2004 wurde Map-Reduce in Form der OpenSource Implementation Hadoop (gemeinsam mit einer Implementation des Google File Systems GFS, u.a.) \footcite{Ghema03} zum de-facto Standard für Big-Data-Analyseaufgaben.\\

Reines Map-Reduce (nach Art von Hadoop) als Programmierparadigma zur Verarbeitung großer Datenmengen zeigt jedoch in vielen Anwendungsfällen Schwächen:
\begin{itemize}
	\item Daten, die in hoher Frequenz entstehen und schnell verarbeitet werden sollen erfordern häufiges Neustarten von Map-Reduce-Jobs. Die Folge ist kostspieliger Overhead durch Verwaltung/Scheduling der Jobs und gegebenenfalls wiederholtem Einlesen von Daten.
	\item Algorithmen die während ihrer Ausführung iterativ Zwischenergebnisse erzeugen und auf vorherige angewiesen sind (häufig bei Maschinenlernalgorithmen) können nur durch persistentes Speichern der Daten und wiederholtes Einlesen zwischen allen Iterationsschritten implementiert werden.
	\item Anfragen an ein solches Map-Reduce-System erfolgen imperativ in Form von kleinen Programmen. Dieses Verfahren ist offensichtlich nicht so intuitiv und leicht erlernbar wie deklarative Abfragesprachen klassischer Datenbanken (z.B. SQL).
\end{itemize}

In der Folge dieser Probleme entstanden viele Ansätze dieses Paradigma zu ersetzen, zu ergänzen oder durch übergeordnete Ebenen und High-Level-APIs zu vereinfachen \footcite{Sin14}.\\

Eine der Alternativen zu der Map-Reduce-Komponente in Hadoop ist die "`general engine for large-scale data processing"' Apache Spark.\\

Ein Indiz für das steigende Interesse an diesem Produkt liefert unter anderem ein Vergleich des Interesses an Hadoop und Spark auf Google:\\

\begin{figure}[h]
\includegraphics[scale=0.6]{bilder/trends_spark_vs_hadoop.PNG}
\caption[Google Trends]{Vergleich der Suchanfragen zu Spark und Hadoop, Stand 24.03.2014 \cite{googletrends}}
\end{figure}

\section{Kontextabgrenzung}
Das Ziel dieser Arbeit ist es eine Analyse und Bewertung der grundlegenden Konzepte und Anwendungsmöglichkeiten von Apache Spark zu vermitteln.

Für ein tieferes Verständnis werden Installation, Cluster-Betrieb und die Entwicklung von Treiberprogrammen beispielhaft durchgeführt, dokumentiert und bewertet. Hierbei kommt Apache Spark Version 1.3.0 zum Einsatz.\\

Nur am Rande wird betrachtet:
\begin{itemize}
	\item Der Vergleich mit ähnlichen Produkten
	\item Die Empirische Messung des Skalierungsverhaltens
	\item Details zu Installation und Betrieb
\end{itemize}

\chapter{Vorstellung von Apache Spark}
Aus Sicht eines Nutzers ist Apache Spark eine API zum Zugriff auf Daten und deren Verarbeitung.\\

Diese API (wahlweise für die Sprachen Scala, Java und Python verfügbar), kann im einfachsten Fall über eine eigene Spark Konsole mit REPL-Pattern\footcite{Hail} verwendet werden.\\
Die Zählung von Wortvorkommen in einem Text - das "`Hello World"' der Big Data Szene - lässt sich dort mit zwei Befehlen realisieren:\\

\begin{lstlisting}[caption=Word Count in der Spark Konsole]
my_dollar ./spark-shell
[...]
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.3.0
      /_/
Using Scala version 2.10.4 (OpenJDK 64-Bit Server VM, Java 1.7.0_75)
Type in expressions to have them evaluated.
[...]
scala> val text = sc.textFile("../Heinrich Heine - Der Ex-Lebendige")
[...]
scala> :paste
text.flatMap(line => line.split(" "))
.map(word => (word, 1))
.reduceByKey(_ + _)
.collect()
[...]
res0: Array[(String, Int)] = Array((Tyrann,,1), (im,2), (Doch,1) ...)

\end{lstlisting}


Aus Sicht eines Administrators ist Apache Spark eine Applikation auf einem Rechencluster, die sich in der Anwendungsschicht befindet und charakteristische Anforderungen insbesondere an Lokalität des Storages und die Netzwerkperformance stellt.\\

Was das konkret bedeutet, welche Mechanismen und Konzepte dahinterstehen und in welchem Ökosystem von Anwendungen sich Apache Spark bewegt wird im nächsten Abschnitt beleuchtet.

\section{Überblick}
\textcolor{gray}{--- Was ist es, wofür ist es, wie funktioniert es, (JVM, Scala) ---}

\begin{figure}[h]
\centering
  \begin{tikzpicture}
  \draw (7,0) node[RectObject, inner xsep=1cm] (Big2) {Big Node};
  \draw (6,2) node[RectObject] (Small2A) {A};
  \draw (8,2) node[RectObject] (Small2B) {B};
  \draw[arrow] (Small2A.north)--(Small2A|-Big2.south);
  \draw[arrow] (Small2B.south)--(Small2B|-Big2.north);
	\begin{umlpackage}{Apache Spark}
    \begin{umlcomponent}{A}
      \umlbasiccomponent{B}
      \umlbasiccomponent[y=-2]{C}
		\end{umlcomponent}
	\end{umlpackage}
  \end{tikzpicture}
  \caption{Verteilung der Software-Komponenten im Cluster}
\end{figure}
\section{Kernkonzepte}
\textcolor{gray}{--- Warum ist Spark schnell, ausfallsicher, secure (und wo vielleicht nicht)? ---}
\textcolor{gray}{--- Annahme: Durchsatzfaktoren RAM, Netzwerk, Festplatte ---}
\subsection{Nutzung von Arbeitsspeicher}
\subsection{Nutzung von persistentem Speicher}
\subsection{Nutzung von CPUs}
\subsection{Scheduling/Shuffling}
\subsection{Kern-API}

\section{Standardbibliotheken}
\textcolor{gray}{--- Warum ist Spark so einfach (und wo vielleicht nicht)? ---}\\
Die vier Standardbibliotheken erweitern die Kern-API für bestimmte, häufig genutzte Aufgaben aus Bereichen der Datenanalyse.\\

Die bedienten Bereiche sind
\begin{itemize}
	\item Deklaratives Abfragen und On-the-Fly-Transformationen (\textit{Spark SQL})
	\item Maschinenlernverfahren (\textit{MLlib})
	\item Echtzeitbehandlung von eingehenden Daten (\textit{Streaming})
	\item Operationen auf Graph-artigen Strukturen (\textit{GraphX})
\end{itemize}

\subsection{Dataframes/Spark SQL}
\subsection{MLlib}
\subsection{Streaming}
\subsection{GraphX}

\section{Betrieb und Security}

\section{Entwicklergemeinschaft}
\textcolor{gray}{--- Herkunft, Apache Foundation, Entwicklungsphilosophien, Anzahl Entwickler, ... ---}\\

\section{Verwandte Produkte}
\textcolor{gray}{--- Ergänzende oder konkurrierende Produkte ---}\\
\subsection{YARN}
\subsection{Mesos}
\subsection{Flink}


\chapter{Spark in der Praxis}
Im Folgenden wird Apache Spark im Rahmen zweier grundsätzlich verschiedener Anwendungsfälle betrachtet. \\

Beispiel 1: Eine typische Anwendung mit verteilten lokalem Storage (HDFS) und Spark als "`Client"' eines bestehenden Yarn Clustermanagers. \textcolor{gray}{--- Commodity Hardware (Rasperry Pi Cluster). ---}\\


Beispiel 2: Eine untypische Anwendung mit verteiltem entfernten Storage und dem Spark Standalone Clustermanager. \textcolor{gray}{--- HPC Hardware ("`Thunder"' des Hamburger KlimaCampus). ---}\\

\section{Echtzeitbewertung von Twitter-Accounts nach ihrer Relvanz für Spark}
\textcolor{gray}{--- Fusion von Tweets und Mailinglisten 
https://spark.apache.org/docs/1.3.0/mllib-feature-extraction.html
Implementation auf einem Raspberry Pi Cluster mit HDFS und Yarn Clustermanager ---}\\
\subsection{Beschreibung des Problems}
\textcolor{gray}{--- Es sollen die beiden Spark Mailingslisten (Developer, User) zur Identifikation relevanter und aktueller Themen genutzt werden. Mit den so bewerteten Begriffen können wiederum Tweets bewertet werden. Mit den Tweets können dann ganze Accounts nach ihrer Relevanz beurteilt werden. ---}\\
\textcolor{gray}{--- Zwei Datenquellen: Tweets (Nahe-Echtzeit), Entwickler-Emails (Sporadisch) ---}\\
\textcolor{gray}{--- Stichworte: HDFS, Yarn, Rasperri Pi Cluster, Machine Learning, Feature Extraction, Big Data Life Cycle ---}\\

\subsection{Hardwarekontext und Performance-Basisdaten}
\textcolor{gray}{--- hier kommen die eingesetzten systeme, und relevante laufzeitmessungen (netzwerk, storage, cpu) hin ---}\\

\begin{figure}[htbp]
  \includesvg[width=\paperwidth]{versuchsaufbau}
	\caption{svg image}
\end{figure}

\begin{table}[ht]
\caption{---DUMMY--- Netzwerkdurchsatz} % title of Table
\centering % used for centering table
\begin{tabular}{c c c c} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Nachrichtengröße & Worker $\rightarrow$ Worker & Master $\rightarrow$ Worker & Worker $\rightarrow$ Master \\ [0.5ex] % inserts table
%heading
\hline % inserts single horizontal line
1kB & 50ms & 837ms & 970ms \\ % inserting body of the table
64kB & 47ms & 877ms & 230ms \\
1MB & 31ms & 25ms & 415ms \\
64MB & 35ms & 144ms & 2356ms \\ [1ex] 
\hline %inserts single line
\end{tabular}
\label{table:nonlin} % is used to refer this table in the text
\end{table}

\subsection{Architekturübersicht}

\begin{center}
\begin{tikzpicture}
\begin{umlpackage}{Apache Spark}
\begin{umlcomponent}{A}
\umlbasiccomponent{B}
\umlbasiccomponent[y=-2]{C}

\umlrequiredinterface[interface=C-interface]{C}
\umlprovidedinterface[interface=B-interface, with port, distance=3cm, padding=2.5cm]{B}
\end{umlcomponent}
\umlbasiccomponent[x=-10,y=1]{D}
\end{umlpackage}
\umlbasiccomponent[x=3,y=-7.5]{E}
\umlbasiccomponent[x=-2, y=-9]{F}
\umlbasiccomponent[x=-7,y=-8]{G}
\umlbasiccomponent[x=-7,y=-11]{H}

\umlassemblyconnector[interface=DA, with port, name=toto]{D}{A}
\umldelegateconnector{A-west-port}{B-west-interface}
\umlVHVassemblyconnector[interface=AE, with port]{A}{E}
\umlHVHassemblyconnector[interface=EF, with port, first arm]{E}{F}
\umlHVHassemblyconnector[interface=GHF, with port, arm2=-2cm, last arm]{G}{F}
\umlHVHassemblyconnector[with port, arm2=-2cm, last arm]{H}{F}

\umlnote[x=-4, y=4, width=3.4cm]{B-west-interface}{Hier ist B-west-interface}
\umlnote[x=2, y=4, width=3.4cm]{C-east-interface}{Hier ist C-east-interface}
\umlnote[x=-8.5, y=-2, width=3.4cm]{toto-interface}{Hier ist toto-interface}
\umlnote[x=-5.5, y=-4.5, width=3.4cm]{A-south-port}{Hier ist A-south-port}
\umlnote[x=-1, y=-6, width=3.4cm]{AE-interface}{Hier ist AE-interface}
\umlnote[x=2, y=-11, width=3.4cm]{F-east-port}{Hier ist F-east-port}
\end{tikzpicture}
\end{center}


\subsection{Detailierte Lösungsbeschreibung}
\textcolor{gray}{--- hier kommen diagramme und codeschnipsel hin ---}\\

\begin{lstlisting}[language=Scala, caption=Treiber für Testanwendung (Programmiersprache Scala)]
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object ScalaApp {
  val my_spark_home = "/home/daniel/projects/spark-1.1.0"

  def main(args: Array[String]): Unit = {
    val logFile = my_spark_home + "/README.md"

    val conf = new SparkConf().setAppName("ScalaApp")
    val sc = new SparkContext(conf)

    val parList1 = sc.parallelize(List(1,2,3,4,5,6))
    val parList2 = sc.parallelize(List(5,6,7,8,9,10))
    val str1 = 
      "RDD1: \%s".format(parList1.collect().deep.mkString(" "))
    val str2 = 
      "RDD2: \%s".format(parList2.collect().deep.mkString(" "))
    val str3 = 
      "# of RDD1: \%s".format(parList1.count())
    val str4 = 
      "Intersect: \%s".format(parList1.intersection(parList2).collect()
    val str5 = 
      "Intersect: \%s".format(parList1.cartesian(parList2).collect()
  }
}
\end{lstlisting}

%%%%%%%%%%%%%%%%%%

\begin{center}
\begin{tikzpicture}
\begin{umlseqdiag}
\umlactor[class=A]{a}
\umldatabase[class=B, fill=blue!20]{b}
\umlmulti[class=C]{c}
\umlobject[class=D]{d}
\begin{umlcall}[op=opa(), type=synchron, return=0]{a}{b}
\begin{umlfragment}
\begin{umlcall}[op=opb(), type=synchron, return=1]{b}{c}
\begin{umlfragment}[type=alt, label=condition, inner xsep=8, fill=green!10]
\begin{umlcall}[op=opc(), type=asynchron, fill=red!10]{c}{d}
\end{umlcall}
\begin{umlcall}[type=return]{c}{b}
\end{umlcall}
\umlfpart[default]
\begin{umlcall}[op=opd(), type=synchron, return=3]{c}{d}
\end{umlcall}
\end{umlfragment}
\end{umlcall}
\end{umlfragment}
\begin{umlfragment}
\begin{umlcallself}[op=ope(), type=synchron, return=4]{b}
\begin{umlfragment}[type=assert]
\begin{umlcall}[op=opf(), type=synchron, return=5]{b}{c}
\end{umlcall}
\end{umlfragment}
\end{umlcallself}
\end{umlfragment}
\end{umlcall}
\umlcreatecall[class=E, x=8]{a}{e}
\begin{umlfragment}
\begin{umlcall}[op=opg(), name=test, type=synchron, return=6, dt=7, fill=red!10]{a}{e}
\umlcreatecall[class=F, stereo=boundary, x=12]{e}{f}
\end{umlcall}
\begin{umlcall}[op=oph(), type=synchron, return=7]{a}{e}
\end{umlcall}
\end{umlfragment}
\end{umlseqdiag}
\end{tikzpicture}
\end{center}


\subsection{Ergebnisse}
\textcolor{gray}{--- Tabellen und Diagramme Ergebnissen, evt. Skalierungsverhalten ---}
\textcolor{gray}{--- Bewertung ---}

\section{Evaluierung einer spark-basierten Implementation von CDOs auf einem HPC Cluster mit nicht-lokalem Storage}
\textcolor{gray}{--- Implementation ausgewählter CDOs (sehr wenige, möglicherweise nur 1-2) mit der Core-API von Spark. Testlauf auf einem HPC Cluster mit nicht-lokalem, allerdings per Infiniband angeschlossenen Storage.
Insbesondere Betrachtung des Skalierungsverhaltens und der "`Sinnhaftigkeit"'. ---}

\subsection{Beschreibung des Problems}
\textcolor{gray}{--- Erläuterung von CDOs (Climate Data Operators). ---}
\subsection{Hardwarekontext und Performance-Basisdaten}
\textcolor{gray}{--- hier kommen die eingesetzten systeme, und relevante laufzeitmessungen (netzwerk, storage, cpu) hin ---}
\subsection{Architekturübersicht}
\textcolor{gray}{--- hier kommen Verteilungs- und Komponentendiagramm hin ---}
\subsection{Detailierte Lösungsbeschreibung}
\textcolor{gray}{--- hier kommen laufzeitdiagramme und codeschnipsel hin ---}
\subsection{Ergebnisse}
\textcolor{gray}{--- Tabellen und Diagramme Ergebnissen, evt. Skalierungsverhalten ---}
\textcolor{gray}{--- Bewertung ---}

\chapter{Schlussbetrachtung}
\section{Kritische W\"urdigung der Ergebnisse}
\section{Ausblick und offene Punkte}


%% appendix if used
%%\appendix
%%\typeout{===== File: appendix}
%%\include{appendix}

% bibliography and other stuff
\backmatter

\typeout{===== Section: literature}
%% read the documentation for customizing the style
%\bibliographystyle{dinat}
%\bibliography{sample}
\printbibliography

%\typeout{===== Section: nomenclature}
%% uncomment if a TOC entry is needed
%\addcontentsline{toc}{chapter}{Glossar}
%\renewcommand{\nomname}{Glossar}
%\clearpage
%\markboth{\nomname}{\nomname} %% see nomencl doc, page 9, section 4.1
%\printnomenclature

\newpage
\glstoctrue
\printglossary[title=Glossar, toctitle=Glossar]

%% index
\typeout{===== Section: index}
\printindex

\HAWasurency

\end{document}
