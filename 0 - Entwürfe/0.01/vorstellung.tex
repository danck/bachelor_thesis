\chapter{Vorstellung und Architekturübersicht von Spark}
Aus Sicht eines Nutzers ist Apache Spark eine \gls{api} zum Zugriff auf Daten und deren Verarbeitung.\\

Diese API (wahlweise für die Programmiersprachen Scala, Java und Python verfügbar), kann im einfachsten Fall über eine eigene Spark Konsole mit \gls{repl} (\cite{Hail}) verwendet werden.\\
Die Zählung von Wortvorkommen in einem Text - das "`Hello World"' der Big Data Szene - lässt sich dort mit zwei Befehlen realisieren (Listing \ref{lst:sconsole_wordcount}).\\

\begin{lstlisting}[caption={Word Count in der Spark Konsole},label={lst:sconsole_wordcount}]
my_dollar ./spark-shell
[...]
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.3.0
      /_/
Using Scala version 2.10.4 (OpenJDK 64-Bit Server VM, Java 1.7.0_75)
Type in expressions to have them evaluated.
[...]
scala> val text = sc.textFile("../Heinrich Heine - Der Ex-Lebendige")
[...]
scala> :paste
text.flatMap(line => line.split(" "))
.map(word => (word, 1))
.reduceByKey(_ + _)
.collect()
[...]
res0: Array[(String, Int)] = Array((Tyrann,,1), (im,2), (Doch,1) ...)
\end{lstlisting}


Aus Sicht eines Administrators oder Softwarearchitekten ist Apache Spark eine Applikation auf einem \gls{cluster}, die sich in der Anwendungsschicht befindet und charakteristische Anforderungen insbesondere an Lokalität des Storages und die Netzwerkperformance stellt.\\

Was das in der konkreten Umsetzung beudeutet und welche Mechanismen und Konzepte dahinterstehen wird in den folgenden Abschnitten dieses Kapitels beleuchtet.

\section{Überblick}
Im Allgemeinen Fall läuft eine Spark-Anwendung auf drei Arten von Rechnern (s. Abb.~\ref{fig:sparkdeployment}):

\begin{enumerate}
	\item \textbf{Clientknoten}\\
	Auf Nutzerseite greift die Anwendung (Treiberprogramm) auf die \gls{api} eines lokalen Spark-Kontextes zu, der die Kontaktdaten eines Clustermanagers sowie verschiedene Konfigurationseinstellungen enthält. 
	
	\item \textbf{\gls{master}knoten}\\
	Der \gls{master}knoten betreibt den \textit{Clustermanager} und ist der Einstiegspunkt in den \gls{cluster}. Hier werden Ressourcenanforderungen des Treiberprogramms an die Arbeitsknoten verteilt und der Betrieb der \textit{Worker} überwacht.
	
	\item \textbf{\gls{worker}knoten}\\
	Die \gls{worker}knoten beherbergen die Spark Workerprozesse. Diese melden verfügbare Ressourcen und erzeugen bei Bedarf \textit{Executors}. \textit{Executors} sind die ausführenden Elemente der Aktionen und Transformationen im Rahmen einer Sparkanwendung. Die \textit{Executors} können untereinander Zwischenergebnisse austauschen und Nachrichten und Ergebnisse an das Treiberprogramm senden.
\end{enumerate}

\begin{figure}[ht!]
	\centering
  \includegraphics[width=0.9\textwidth]{sparkdeployment2.pdf}
	\caption{Verteilungsdiagramm einer typischen Sparkinstallation}
	\label{fig:sparkdeployment}
\end{figure}

Um die Architektur und Optimierungskonzepte eines verteilten Systems bewerten zu können ist es wichtig, welche Eigenschaften der unterliegenden Hardware angenommen werden.\\

Spark ist explizit für den Betrieb innerhalb eines Hadoop/YARN \gls{cluster}s geeignet. Hadoop/YARN wiederum ist für den Betrieb auf einem \gls{cluster} auf Mittelklasse-Mehrzweckmaschinen (Commodity Hardware) optimiert (\cite{Mer14}). Für Spark kann daher von einer vergleichbaren Hardwarekonfiguration ausgegangen werden.\\

Der Vergleich von drei aktuellen Rack Servern der 2000-Euro-Klasse in der Grundausstattun) - hier als Mittelklasse-Geräte bezeichnet - liefert die folgenden Verhältnisse der wesentlichen Schnittstellen zueinander (Siehe Anhang~\ref{subsec:commodity_servers}).

\begin{table}[ht]
	\centering % used for centering table
	\begin{tabular}{c c c} % centered columns (4 columns)	
		\hline\hline %inserts double horizontal lines
		Netzwerk & Festspeicher & Arbeitsspeicher\\ [0.5ex] % inserts table
		%heading
		\hline % inserts single horizontal line
		0,125 GB/s & 1 GB/s & 17 GB/s \\ % inserting body of the table
		\hline %inserts single line
	\end{tabular}
	\caption{Theoretische Spitzenleistungen bei Mittelklasse-Servern} % title of Table
	\label{table:vgldurchsatz} % is used to refer this table in the text
\end{table}

Bei den folgenden Bewertungen der Kernkonzepte ist es wichtig sich die aus Tabelle \ref{table:vgldurchsatz} abgeleiteten Größenordnungen des Durchsatzes (\textit{D}) der verschiedenen Datenkanäle zu vergegenwärtigen:

\begin{equation*}
	D_{Netzwerk} < D_{Festspeicher} < D_{Arbeitsspeicher}
\end{equation*}

Für eine effiziente Verarbeitung von Daten ist es - ganz allgemein - also wünschenswert den größten Anteil des Datentransfers im Arbeitsspeicher zu haben, einen kleineren Anteil auf der Festplatte und einen noch kleineren Anteil auf Netzwerkverbindungen.\\

Es ist das wichtigste Ziel der folgenden Kernkonzepte von Apache Spark unter diesen Bedingungen die effiziente und stabile Verarbeitung \textit{großer Datenmengen} (\cite{Sam14}) zu gewährleisten.\\

\section{Kernkonzepte}

\subsection{Resilient Distributed Datasets}
Die universelle Einheit mit der Daten auf Spark repräsentiert werden ist eine verteilte Datenstruktur, ein sogenanntes \gls{rdd}\cite{Mat12}.\\

Ein Beispiel für ein solches \gls{rdd} wurde bereits erwähnt, nämlich das in Listing \ref{lst:sconsole_wordcount} erzeugte Objekt \lstinline|text|:\\

\begin{lstlisting}[language=Scala]
val text = sc.textFile("../Heinrich Heine - Der Ex-Lebendige")
\end{lstlisting}

\Glspl{rdd} entstehen nicht nur durch Einlesen externer Datenquellen, sondern können auch explizit von einem Treiberprogramm erzeugt werden:\\

\begin{lstlisting}
val listRDD = sc.parallelize(List(1,2,3,4,5,6))
\end{lstlisting}

Die gesamte operative Kern-\gls{api} dreht sich um die Steuerung dieser Datenstruktur. Insbesondere sind auch die in den Standardbibliotheken verfügbaren "`höheren"' \glspl{api} auf diesen \glspl{rdd} implementiert.

Sie sind damit die wichtigste Abstraktion des Applikationskerns.\\

In erster Näherung können \glspl{rdd} als eine Variante von \gls{dsm} (\cite{Nitzberg:1991:DSM:112827.112855}, \cite{Mat12}) verstanden werden. Sie haben allerdings sehr charakteristische Einschränkungen und Erweiterungen, die in diesem Kapitel erläutert werden.\\

\paragraph{Verteilungssicht}\\

\\
Aus Verteilungssicht ist ein \gls{rdd} ein Datensatz, der über den Arbeitsspeicher mehrerer Maschinen partitioniert ist (Abb.~\ref{fig:rdds1}).

\begin{figure}[ht!]
	\centering
  \includegraphics[width=0.9\textwidth]{RDDs1.pdf}
	\caption{Resilient Distributed Datasets aus Verteilungssicht}
	\label{fig:rdds1}
\end{figure}

\paragraph{Laufzeitsicht}\\

\\
\Glspl{rdd} sind nicht veränderbar. Es ist nicht möglich Operationen gezielt auf einzelnen Elementen eines \gls{rdd} anzuwenden. Stattdessen ist es nur möglich ein einmal definiertes \gls{rdd} durch globale Anwendung von Operationen in ein anderes zu überführen.\\
Solche globalen - auf sämtlichen Partitionen des \gls{rdd} durchgeführten - Operationen können zwar ihren Effekt auf einzelne Elemente eines \gls{rdd} beschränken, die Ausführung erfolgt jedoch in jedem Fall auf allen Partitionen.\\

Eine Folge von Operationen $op_1op_2op_3...$ wird als \textit{Lineage} eines \gls{rdd} bezeichnet. Die \textit{Lineage} kann als das "`Rezept"' zur Erstellung eines Datensatzes verstanden werden.

Dabei gibt es zwei grundsätzlich verschiedene Operationen, nämlich \textit{Transformationen} und \textit{Aktionen}.\\

\textbf{\textit{Transformationen}} sind Operationen, die ein \gls{rdd} auf ein anderes abbilden:\\
\[Transformation: RDD \times RDD\ \longrightarrow RDD\]
oder
\[Transformation: RDD\ \longrightarrow RDD\]
\\
Es wird also - grob gesagt - nur die abstrakte Repräsentation des Datensatzes geändert, ohne tatsächlich dessen Datenelemente für den Programmfluss im Treiberprogramm abzurufen.\\
Beispiele für solche Operationen sind:
\begin{itemize}
	\item \textit{filter}
	\item \textit{join}
\end{itemize}\\

\textbf{\textit{Aktionen}} sind Operationen, die \glspl{rdd} in eine andere Domäne abbilden:\\
\[Action: RDD \longrightarrow Domain_x, Domain_x \neq RDD\]
\\
Beispiele für Aktionen sind die Methoden:
\begin{itemize}
	\item \textit{reduce}
	\item \textit{count}
	\item \textit{collect}
	\item \textit{foreach}
\end{itemize}\\

Einigen dieser Operationen, kann im Sinne des Command-Patterns (\cite{FPP13}) eine Funktion bzw. ein Funktionsobjekt übergeben werden, dass die gewählte Operation spezifiziert.

Solange nur \textit{Transformationen} auf einem \gls{rdd} ausgeführt werden, ist dieses noch ein bloßes "`Rezept"' zur Erstellung eines Datensatzes. Tatsächlich wurde noch kein Speicher reserviert und der Cluster wurde noch nicht aktiv (\cite{Mat12}):\\

\begin{figure}[ht!]
	\centering
  \includegraphics[width=0.6\textwidth]{rdds_no_action.pdf}
	\caption{RDD Lineage vor Aktion (gestrichelte Linie steht für \textit{nicht initialisiert})}
	\label{fig:rdds_no_action}
\end{figure}

Sobald die erste \textit{Aktion} aufgerufen wird, werden die Transformationen nach der vorgegebenen Reihenfolge ausgeführt und schließlich die geforderte \textit{Aktion}. Die Initialisierung des \gls{rdd} erfolgt also "`lazy"':\\

\begin{figure}[ht!]
	\centering
  \includegraphics[width=0.9\textwidth]{rdds_action.pdf}
	\caption{RDD Lineage nach Aktion}
	\label{fig:rdds_action}
\end{figure}

Wie in Abb. \ref{fig:rdds_action} dargestellt ist, werden während der Transformationsvorgänge keine Zwischenergebnisse gespeichert. Möchte man Zwischenergebnisse zu einem späteren Zeitpunkt oder in anderem Zusammenhang wiederverwenden, kann man dies explizit über das Kommando \lstinline|persist()| oder \lstinline|cache()| anweisen:\\

\begin{figure}[ht!]
	\centering
  \includegraphics[width=0.9\textwidth]{rdds_action_persist.pdf}
	\caption{RDD Lineage nach Aktion und mit Persist()}
	\label{fig:rdds_action_persist}
\end{figure}
\\
Realisiert ist das Konzept der \textit{Lineage} und der "`Lazy Initialization"' von \glspl{rdd} durch Transformations-Methoden, die eine Variante des Factory-Pattern (\cite{FPP13}) implementieren. Die erzeugten Objekte sind dabei wiederum Unterklassen von \gls{rdd}:\\
Jedes \gls{rdd}-Objekt führt eine Liste von Vorgängern mit. Aus dieser lässt sich auch die Art der Berechnung des jeweiligen Nachfolgers ableiten.\\

Jede weitere Transformations-Methode konstruiert nun lediglich eine neues \gls{rdd}-Objekt. Dieses basiert auf dem aktuellen Objekt und der jeweiligen Transformation.\\

Ein Beispiel für solch eine Transformation auf einem \gls{rdd} ist die Methode \textit{map}\footnote{https://github.com/apache/spark/blob/branch-1.3/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L285 (abgerufen am 30.05.2015)} (Listing \ref{lst:spark_map}).\\

In den Zeilen 7 und 8 ist zu sehen, wie ein neues \gls{rdd} erzeugt wird und diesem das aktuelle \gls{rdd} zusammen mit der Funktion \lstinline|f| (bzw. \lstinline|cleanF|) übergeben wird. Diese Funktion beschreibt die Erzeugung der Elemente des neuen \gls{rdd} aus den Elementen des aktuellen.\\

\begin{lstlisting}[language=Scala,caption={Map-Methode aus org.apache.spark.rdd.RDD v1.3.0},label={lst:spark_map}]
  /**
   * Return a new RDD by applying a function to all elements of 
	 * this RDD.
   */
  def map[U: ClassTag](f: T => U): RDD[U] = {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) 
		  => iter.map(cleanF))
  }
\end{lstlisting}
\\
Die tatsächliche Berechnung eines \glspl{rdd} wird dann bei dem Aufruf einer Aktion gestartet. Als Beispiel hierfür sei die Methode \lstinline|foreach|\footnote{https://github.com/apache/spark/blob/branch-1.3/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L793 (abgerufen am 30.05.2015)} aufgeführt (Listing \ref{lst:spark_foreach}).\\
In Zeile 6 wird auf dem Spark-Context die Methode \lstinline|runJob| aufgerufen, die die Aufgabe wiederum an den Scheduler delegiert. Dort werden dann die rekursiven Abhängigkeiten des aktuellen \gls{rdd} aufgelöst und - je nach Konfiguration - die Tasks verteilt.\\

\begin{lstlisting}[language=Scala,caption={foreach-Methode aus org.apache.spark.rdd.RDD v1.3.0},label={lst:spark_foreach}]
  /**
   * Applies a function f to all elements of this RDD.
   */
  def foreach(f: T => Unit) {
    val cleanF = sc.clean(f)
    sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF))
  }
\end{lstlisting}
\\
Das Konzept der \textit{Lineage} ist zentral für die Fehlertoleranz der \gls{rdd}s:
Geht eine Partition verloren - beispielsweise durch Defekt eines Knotens - ist das "`Rezept"' zur Erstellung des Datensatzes in der \textit{Lineage} des \gls{rdd}-Objektes weiterhin vorhanden und die Partition kann gezielt wiederhergestellt werden.\\

Ein weiterer Vorteil dieser Art von Arbeitsdatensatz wird ebenfalls sofort deutlich: Im optimalen Fall sind die zu ladenden Daten von jedem der \gls{worker} auf unabhängigen Kanälen erreichbar (z.B. auf dem lokalen Festspeicher) und gleichmäßig auf diesen Kanälen partitioniert (Abb.~\ref{fig:rdds2}.\\

\begin{figure}[h!]
	\centering
  \includegraphics[width=0.9\textwidth]{RDDs2.pdf}
	\caption{Resilient Distributed Datasets mit Datenquelle aus Verteilungssicht}
	\label{fig:rdds2}
\end{figure}

Im diesem Fall ergäbe sich mit einer Anzahl \gls{worker} $n$ und einem Durchsatz $\delta$ zu der jeweiligen Datenquelle also ein idealer Gesamtdurchsatz beim Einlesen von Daten von:

\begin{equation}
	\sum_{i=1}^{n} \delta_i
\end{equation}

Kommen in einem Anwendungsfall \gls{rdd}s zum Einsatz, deren Elemente einzeln über eine oder mehrere Operationen untereinander verknüpft werden, kann es sinnvoll sein diese schon im Vorfeld der Verarbeitung entsprechend erwarteter Cluster zu partitionieren. Cluster sind hierbei Teilmengen des \gls{rdd}, die mit besonders hoher Wahrscheinlichkeit oder besonders häufig untereinander verknüpft werden.\\

Dadurch kann ein größerer Teil der Operationen auf den einzelnen Datensätzen bereits lokal auf dem Knoten der jeweiligen Partition durchgeführt werden. Die Netzwerklast bei dem anschließenden \textit{Shuffle} der Daten (siehe Abschnitt 2.2.2) fällt dann geringer aus.\\

Solch eine Partitionierung kann - entsprechende Erwartung über das Verhalten der Verarbeitung vorausgesetzt - mit einem maßgeschneiderten Partitionierer erreicht werden (Abb.~\ref{lst:partitioner}), der dann dem betroffenen \gls{rdd} übergeben wird.\\

\begin{lstlisting}[language=Scala,caption={Beispiel: Minimaler Partitionierer},label={lst:partitioner}]
/*
 * Beispiel fuer einen minimalen Partitionierer. 
 * Ueber selbstdefinierte Hash Codes kann hier eine 
 * massgeschneiderte Verteilung ueber die 
 * Knoten erreicht werden.
 */
class MinimalPartitioner extends Partitioner {
 def numPartitions = 10

 def getPartition(key: Any): Int =
   key.hashCode % numPartitions
	
 def equals(other: Any): Boolean =
   other.isInstanceOf[MinimalPartitioner]
}
\end{lstlisting}

\subsection{Scheduling/Shuffling}\\

Dieser Abschnitt vertieft die Betrachtung des Berechnungsmodells von Spark. Mit Berechnungsmodell ist die Art gemeint, wie mit den High-Level \glspl{api} und den bisher vorgestellten Komponenten die verteilte Berechnung realisiert wird.\\
Dabei werden insbesondere die Begriffe \textit{\textbf{Task}}, \textit{\textbf{Job}} und \textit{\textbf{Stage}} näher erläutert.\\

Bei den \glspl{rdd} wurden bisher insbesondere drei Aspekte behandelt:
\begin{itemize}
	\item Die \textbf{Partitionierung} der Elemente über verschiedene Rechner
	\item Die \textbf{Vorgänger} eines \gls{rdd} bezüglich dessen \textit{Lineage}
	\item Die \textbf{Funktion} mit der ein \gls{rdd} aus einem oder mehreren direkten Vorgängern berechnet wird
\end{itemize}

Betrachtet man nur die Vorgänger-Beziehung, dann erhält man zunächst eine einfache Baumstruktur für die Berechnung eines \gls{rdd} (Abb.~\ref{fig:lineage_basic}).\\

\begin{figure}[ht!]
	\centering
  \includegraphics[width=0.9\textwidth]{lineage_basic.pdf}
	\caption{Beispiel eines einfachen Abhängigkeitsbaums eines RDD}
	\label{fig:lineage_basic}
\end{figure}

Betrachtet man zusätzlich die Partitionierung und die Funktion mit der \glspl{rdd} transformiert werden, sieht man einen wichtigen Unterschied zwischen verschiedenen Vorgängerbeziehungen (siehe Abb.~\ref{fig:lineage_parted}):
\begin{itemize}
\item Solche deren einzelne Partitionen höchstens eine Vorgängerpartition haben
\item Solche bei denen mindestens eine Partition mehr als eine Vorgängerpartition hat
\end{itemize}

\begin{figure}[ht!]
	\centering
  \includegraphics[width=0.9\textwidth]{lineage_parted.pdf}
	\caption{Gerichteter azyklischer Graph der Abhängigkeiten auf Partitionen}
	\label{fig:lineage_parted}
\end{figure}

Wird eine Partition aus höchstens einer anderen erzeugt, lässt sich diese direkt auf dem selben Knoten berechnen. Werden jedoch verschiedene Partitionen benötigt, um eine Folgepartition zu erzeugen, stellt sich die Frage auf welchen Knoten das am Besten geschieht.\\
Spark unterteilt diese beiden Fälle in \textit{narrow dependencies} (erster Fall) und \textit{wide dependencies} (zweiter Fall) (\cite{Mat12}).\\

Der Abhängigkeitsgraph eines \gls{rdd} wird nun in sogenannte \textit{Stages} zerlegt (Abb.~\ref{fig:lineage_stages}). Eine \textit{Stage} ist dabei ein Untergraph, dessen Elemente (von den Blättern in Richtung Wurzel betrachtet) auf eine gemeinsame \textit{wide dependency} stoßen.\\

\begin{figure}[ht!]
	\centering
  \includegraphics[width=0.9\textwidth]{lineage_stages.pdf}
	\caption{Stages als Untermenge des Ahängigkeitsgraphen von Partitionen}
	\label{fig:lineage_stages}
\end{figure}

Elemente innerhalb einer Stage, können nach dieser Konstruktion unabhängig auf den Knoten berechnet werden, die die jeweilige Partition vorhalten. Ein Datenelement \(x_{i,j}\) sei dabei Element eines \gls{rdd} \(j\) und Element einer lokalen Partition \(P_{i,j}\) (d.h. es existiert auf einem Knoten \(i\)):

\[x_{i,j} \in Partition_{i,j} \subseteq RDD_j\]

Weil nach Definition der \textit{narrow dependencies} innerhalb einer Stage die Elemente einer Partition \(P_{i,j}\) aus den Elementen genau einer Vorgängerpartition \(P_{i,j-1}\) berechnet werden können, lässt sich jedes Element \(x_{i,j}\) des Nachfolger-\gls{rdd} über eine Komposition lokaler Transformationen \(trans_{i,k}, k = 0..j\) berechnen:\\ 

\[x_{i,j} = (trans_{i,j} \circ trans_{i,j-1} \circ ... \circ trans_{i,1}\circ trans_{i,0})(x_{i,0})\]

Für den Übergang zwischen Stages ist die Berechnung der Nachfolger etwas aufwändiger. Hier stammen - nach Definition der \textit{wide dependencies} - die direkten Vorgänger eines Elementes nicht alle aus der selben Partition.\\

Um eine neue Partition aus mehreren Vorgänger-Partitionen zu erzeugen werden zunächst geeignete Ausführungsorte\footnote{Geeignete Ausführungsorte (\textit{preferred locations}) können sich z.B. aus dem Ort eines Blocks bei HDFS ermitteln lassen (Node-Local, Rack-Local, etc.) oder daraus, ob ein Executor bereits einen Datensatz geladen hat (Process-Local)} ermittelt, von denen dann jeder per geeignetem Partitionierer mindestens einen \textit{bucket} von Elementen verarbeitet. Diese Elemente stammen dann in der Regel aus verschiedenen Vorgängerpartitionen.\\
Das so neu-partitionierte und verarbeitete \gls{RDD} wird so zum Ausgangspunkt des folgenden \gls{rdd}.\\
Eine Schlüsselrolle kommt bei diesem Prozess der Methode \lstinline|runTask| in der Klasse \lstinline|ShuffleMapTasks|\footnote{https://github.com/apache/spark/blob/branch-1.3/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala, (abgerufen am 30.05.2015)} aus \textit{org.apache.spark.scheduler.ShuffleMapTask} zu.\\

\subsection{Anwendungsdeployment}\\

Die Komponente einer Anwendung, die von der Spark API Gebrauch macht, wird in der Spark-Terminologie als \textit{Treiberprogramm} bezeichnet. \\

Es gibt verschiedene Szenarien des Deployments. Das Treiberprogramm kann grundsätzlich auf zwei verschiedene Arten gestartet werden:\\

\begin{enumerate}
	\item Übermittlung des Treibers als kompiliertes Package (z.B. als \gls{jarfile}) mit statischer Verlinkung aller erforderlichen Bibliotheken (Ausnahmen sind Bibliotheken die auf allen Knoten bereits verfügbar sind, z.B. Spark, Hadoop, etc.). Standardbibiotheken von Spark und Konfigurationseinstellungen wie z.B. die Angabe des \textit{Clustermanagers} können zur Startzeit des Treibers durch das Spark \textit{Submission-Skript} erfolgen.
	\item Start eines eigenständig lauffähigen Treibers mit vollständig konfigurierter und verlinkter Spark-Umgebung und expliziter Angabe eines \textit{Clustermanagers}.
\end{enumerate}

Der zweite Fall ist eher exotisch, weil eine derart enge Kopplung zwischen dem Treiber und der Konfiguration des Clusters aus Wartungsgründen nicht wünschenswert ist.\\

Im ersten Fall ergeben sich zwei weitere Möglichkeiten zum Ort der Ausführung des Treibers:\\

\begin{enumerate}
	\item \textbf{Client-Modus}
	Der Treiber wird direkt auf dem Host (Gateway-Rechner) ausgeführt auf dem der Treiber übermittelt wurde (Abb.~\ref{fig:spark_deployment_clientmode}). Tatsächlich wird er sogar innerhalb des Submission-Skript-Prozesses gestartet \textbf{(VERWEIS)}.
	\item \textbf{Cluster-Modus}
	Der Treiber wird von dem Gateway-Rechner an einen \gls{worker} des Clusters übertragen und dort ausgeführt (Abb.~\ref{fig:spark_deployment_clustermode}).
\end{enumerate}\\

\begin{figure}[ht!]
	\centering
  \includegraphics[width=\textwidth]{spark_deployment_clientmode.pdf}
	\caption{Application Deployment im Client Modus}
	\label{fig:spark_deployment_clientmode}
\end{figure}\\

\begin{figure}[ht!]
	\centering
  \includegraphics[width=\textwidth]{spark_deployment_clustermode.pdf}
	\caption{Application Deployment im Cluster Modus}
	\label{fig:spark_deployment_clustermode}
\end{figure}\\

Die Lokalität eines Treibers (der mit den Executors auf den \gls{worker}n kommunizieren muss) kann Einfluss auf Laufzeit und Latenzverhalten des Programms haben.\\

Im Fall eines clusterfernen Gateway-Rechners kann also Treiberdeployment im Clustermodus sinnvoll sein, die Standardeinstellung ist jedoch der Clientmodus (siehe auch \cite{spark_submission}).\\

Abb.~\ref{fig:app_deployment_process} zeigt das Sequenzdiagramm eines typischen Deploymentprozesses, wie er auch im praktischen Teil dieser Arbeit zum Einsatz kommt.

\begin{figure}[ht!]
	\centering
  \includegraphics[width=\textwidth]{spark_deployment_process.pdf}
	\caption{Laufzeitdiagramm einer Spark-Anwendung im Client Modus (vereinfacht)}
	\label{fig:app_deployment_process}
\end{figure}

\section{Standardbibliotheken}
Die vier Standardbibliotheken erweitern die Kern-API für bestimmte, häufig genutzte Aufgaben aus Bereichen der Datenanalyse.\\

Dieser Abschnitt gibt einen Überblick der Bibliotheken und deren zentrale Konzepte zur Abstraktion von \glspl{rdd} für die spezifischen Zwecke.\\

\subsection{SparkSQL}
Die Spark SQL Bibliothek bietet Schnittstellen für den Zugriff auf strukturierte Daten. Dazu wird eine weitere zentrale abstrakte Datenstruktur neben den \glspl{rdd} eingeführt. Diese Datenstruktur sind \glspl{dataframe} wie sie in ähnlicher Art auch aus der Programmiersprache R\footnote{http://www.r-project.org/, abgerufen am 20.05.2015} bekannt sind.\\

\Glspl{dataframe} und die SparkSQL Bibliothek bilden einen Ansatz, relationale Abfragen mit komplexen prozeduralen Algorithmen zu verknüpfen, um die Performance bei Abfragen auf großen Datenmengen zu optimieren (Vgl. \cite{Armbrust:2015:SSR:2723372.2742797}).\\

Die Abfrage und Verarbeitung von Daten kann entweder über die Schnittstellen dieses Datentyps erfolgen (ähnlich wie bei \glspl{rdd} oder in Form von SQL-Syntax.\\

Diese Form des Zugriffs ist beispielsweise über einen JDBC-Adapter möglich und bietet so anderen Anwendungen und Frameworks die Möglichkeit bestehende SQL-Schnittstellen mit Apache Spark zu verbinden.\\

Ähnlich wie bei \glspl{rdd} werden komplexe Folgen von Abfragen zunächst in dem \gls{dataframe}-Referenzobjekt gespeichert, und erst bei Initialisierung ausgewertet. Dabei wird eine Optimierungspipeline genutzt, die zunächst die relationalen Ausdrücke optimiert und ihn dann in Spark-Operationen auf \glspl{rdd} übersetzt.\\

Die SparkSQL-Bibliothek zerfällt dabei in vier Unterprojekte\footcite{https://github.com/apache/spark/tree/branch-1.3/sql, abgerufen am 16.06.2015}. Die ersten beiden sind essentiell für den Mechnismus zum Prozessieren von Dataframes als eine Prozess auf \glspl{rdd}:
\begin{enumerate}
	\item \textbf{Catalyst}: Eine Optimierungsengine für relationale Ausdrücke
	\item \textbf{Core}: Die Benutzer-API und Ausführungsumgebung für optimierte Catalyst-Ausdrücke auf dem Apache Spark Applikationskern
	\item \textbf{Hive}: Ein Adapter für die Datawarehouse-Software Apache Hive\footnote{https://hive.apache.org/, abgerufen am 16.06.2015}
	\item \textbf{Thriftserver}: Eine Portierung des Hive-Servers auf Apache Spark
\end{enumerate}

\subsection{MLlib}
Die Spark \textit{Machine Learning Library} bietet Implementationen bewährter Maschinenlernverfahren und Abstraktionen deren Einsatz.\\
Dabei werden die Komponenten eines Maschinenlernprogramms in \textit{Transformer}, \textit{Estimator} und \textit{Pipelines} aufgeteilt\footnote{https://spark.apache.org/docs/1.3.1/ml-guide.html, abgerufen am 16.06.2015}. Als Abstraktion der verarbeiteten Datensätze werden \glspl{dataframe} aus der SparkSQL-Bibliothek verwendet.\\

Die Komponenten bezeichnen folgendes:
\begin{itemize}
	\item \textbf{Transformer} sind zustandslose Objekte, mit einer \lstinline|transform|-Methode \glspl{dataframe} verändern.
	\item \textbf{Estimator} erzeugen Transformer aus einem Eingangsdatensatz (\gls{dataframe}). Sie sind ebenfalls zustandslos und können als Abstraktion eine Lernalgorithmus verstanden werden.
	\item \textbf{Pipelines} sind in der MLlib eine Abstraktion der Verknüpfung von Transformern und Estimatorn. Ähnlich wie bei den Transformationen eines \gls{rdd}, werden auch dies "`lazy"' initialisiert und in Stages aufgeteilt.
\end{itemize}

Teile der Spark MLlib werden im praktischen Teil dieser Arbeit noch näher erläutert.\\

\subsection{Streaming}
Die Spark Streaming Bibliothek ermöglicht die Verarbeitung von Datenströmen mit dem Konzept der \glspl{rdd}.\\

Dabei werden werden \glspl{rdd} zu diskretisierten Datenströmen (sogenannte \textit{DStreams}) erweitert.\\

Dabei werden ein oder mehrere Empfänger (\textit{Receiver}) auf Workerknoten eines Sparkclusters gestartet, die anschließend eingehende Daten über ein vorgegebenes Zeitfenster puffern. Die Puffer können dabei verteilt auf mehreren verfügbaren Workerknoten angelegt werden.\\

Nach Ablauf des Zeitfensters bilden diese Puffer die Partitionen eines RDD mit den empfangenen Datensätzen. Dieses \gls{rdd} wiederum ist Teil einer Sequenz von \glspl{rdd} die im Verlauf dieses Prozesses erzeugt werden.\\

Mit diesem Verfahren wird ein Großteil der Operationen klassischer \gls{rdd} auch für Datenströme verfügbar.\\

Eine Reihe von Empfängern für solche Datenströme sind bereits in der Spark Streaming Bibliothek enthalten (z.B. für Kafka\footnote{http://kafka.apache.org/, abgerufen am 16.06.2015}, Twitter, ZeroMQ\footnote{http://zeromq.org/, abgerufen am 16.06.2015}, ...)\footnote{https://spark.apache.org/docs/1.3.1/streaming-programming-guide.html, abgerufen am 16.06.2015}. Weitere Empfänger können selbst implementiert und übergeben werden.\\

Eine Beispielimplementation dieses Verfahrens wird im praktischen Teil durchgeführt.

\subsection{GraphX}
Die GraphX Bibliothek implementiert Graphstrukturen auf Grundlage von \glspl{rdd} und bietet Methoden zu deren Verarbeitung.\\

Die Graphstrukturen sind dabei sogenannte Property-Graphen. Property-Graphen (wie sie beispielsweise auch aus der Datenbank Neo4j\footcite{http://neo4j.com/, abgerufen am 16.06.2015} bekannt sind) sind gerichtete Multigraphen, deren Kanten-Objekte mit Eigenschaften versehen werden können.\\

Die Graphen der Spark GraphX-Bibliothek bilden dabei Tupel aus \glspl{rdd} wobei das erste die Ecken und das zweite die Kanten des Graphen enthält.\\

Die Zerteilung eines Graphen über mehrere Maschinen erfolgt nach dem sogenannten \textit{Edge Cut} Verfahren (\cite{Gonzalez:2014:GGP:2685048.2685096}). Dabei werden die Graphen - logisch gesehen - entlang der Kanten geteilt.\\

Eine einzelne Ecke kann daraufhin auf mehreren Maschinen gleichzeitig liegen. Aus der ID jeder dieser Kopien, lässt sich jedoch die Partition (im Sinne des darunterliegenden \gls{rdd}) einer Kante stets eindeutig berechnen.\\

Die Attribute der Ecken werden ebenfalls in den Kanten gespeichert. Um Änderungen an alle Kopien einer Ecke zu propagieren wird zusätzlich zu den \glspl{rdd} der Ecken und Kanten noch eine Routingtabelle geplegt. Über diese Routing Tabelle werden die Replikate von Ecken verfolgt und bei Änderungen einer Ecke alle inzidenten Kanten entspechend angepasst.\\

\section{Entwicklergemeinschaft}

Apache Spark begann als Entwicklung einer Gruppe von Forschern der University of California, Berkely. Spark basiert auf einer Implementation der von dieser Gruppe untersuchten \glspl{rdd}\cite{Mat12}. Als wesentlicher Meilenstein der Entwicklung von Apache Spark, kann die Veröffentlichung eines gemeinsamen Papers der Forschungsgruppe um Matei Zaharia im Jahr 2012 gelten.\\

Seit Februar 2014 (\cite{apacheblog}) ist Spark ein Top-Level Projekt der Apache Software Foundation (\cite{apache}) und wird dort unter der Apache License 2.0 (\cite{apachelic}) weiterentwickelt.\\

Eine Übersicht der verantwortlichen Entwickler kann unter \cite{committer} eingesehen werden.
Zum Zeitpunkt dieser Arbeit gehören u.a. Entwickler von Intel, Yahoo! und Alibaba zu den Stammentwicklern.\\

Die Kommunikation innerhalb der Entwickler- und Anwendergemeinschaft findet wesentlich in den offiziellen Mailinglisten (Abb. \ref{fig:mailinglisten}) und dem Issue-Tracker (\cite{issuetracker}) der Apache Software Foundation statt.

\begin{figure}[ht!]
	\centering
	\begin{tikzpicture}
	\begin{axis}[
			title={},
			xlabel={Monat},
			ylabel={Nachrichten pro Tag},
			%xmin=0, xmax=100,
			ymin=0, ymax=100,
			symbolic x coords={2014/11,2014/12,2015/01,2015/02,2015/03},
			ytick={0,20,40,60,80,100},
			legend pos=north west,
			ymajorgrids=true,
			grid style=dashed,
	]
	 
	\addplot[
			color=blue,
			mark=square,
			]
			coordinates {
			(2014/11,16.43)
			(2014/12,13.35)
			(2015/01,12.06)
			(2015/02,15.89)
			(2015/03,14.61)
			};

	\addplot[
			color=red,
			mark=diamond,
			]
			coordinates {
			(2014/11,64.43)
			(2014/12,59.58)
			(2015/01,67.64)
			(2015/02,75.64)
			(2015/03,80.58)
			};
			\legend{dev@spark.apache.org,user@spark.apache.org}

	 
	\end{axis}
	\end{tikzpicture}
	\caption{Aktivität auf den offiziellen Spark Mailinglisten}
	\label{fig:mailinglisten}
\end{figure}
\\

\section{Übersicht verwandter Produkte}
Um Spark besser im Bereich bestehender Lösungen einzuordnen werden im Folgenden einige Produkte genannt die häufig zusammen mit Spark verwendet oder ähnliche Aufgaben erfüllen.

\paragraph{Hadoop/YARN}\\

Hadoop\footnote{https://hadoop.apache.org/, abgerufen am 16.06.2015} lässt sich als eine Art Betriebssystem für Cluster zur Datenanalyse beschreiben. Zu den wesentlichen Komponenten zählen ein Dateisystem (HDFS) ein Datenverarbeitungsmodell (MapReduce) und ein Scheduler (YARN).
Alle genannten Komponenten sind für den fehlertoleranten und skalierbaren Betrieb auf verteilten Hardware-Komponenten vorgesehen.\\

Zwar wird mit MapReduce auch eine Komponente zur Verarbeitung von verteilt gespeicherten Daten zu Verfügung gestellt, andere Datenverarbeitungsmodelle können jedoch gleichberechtigt und unter Aufsicht des Ressourcenschedulers YARN betrieben werden.\\

Spark liefert eine mögliche Implementation eines solchen alternativen Datenverarbeitungsmodells.

\paragraph{Mesos}\\

Apache Mesos\footnote{http://mesos.apache.org/, abgerufen am 16.06.2015} ist seit Beginn der Entwicklung von Spark ein optionaler Clustermanager für Sparkapplikationen \cite{Mat12}.\\

Als reiner Clustermanager ersetzt Mesos die Spark Master-Komponente in der Funktion des knotenübergreifenden Ressourcenmanagements.\\

Wie bei YARN ermöglicht dies auch anderen Anwendungen die über Mesos verwaltet werden einen gleichberechtigten Betrieb auf dem selben Cluster.\\

\paragraph{Flink}\\

Apache Flink\footnote{https://flink.apache.org/, abgerufen am 16.06.2015} ist mit seiner Funktionalität ein direkter Konkurrent zu Apache Spark.\\

Flink ist ebenfalls in Java/Scala geschrieben und bietet mit drei Standardbibliotheken zur Graphprozessierung, zum Maschinenlernverfahren und zu SQL-artigen Abfragen auf strukturierten Datensätzen einen sehr ähnlichen Funktionsumfang.\\

Der wesentliche Unterschied in der Architektur von Flink verglichen mit Spark liegt im Applikationskern. Flink bietet ein Streaming-Backend auf dem wiederum auch Batchprozesse implementiert werden können. Spark wiederum bietet ein Batch-Processing-Backend (Operationen auf \glspl{\rdd}) auf dem auch Streaming-Prozesse implementiert werden können.\\

Auffällig ist weiterhin, dass im Flink Applikationskern häufig auf Low-Level-Methoden zurückgegriffen wird und beispielsweise mit der Endianness\footnote{Art der Bytereihenfolge bei der Ablage von Daten im Arbeitsspeicher} auf verschiedenen Speicherarchitekturen umgegangen wird. Solch feingranulare Optimierungsbemühungen sind dem Autoren bei Spark nicht bekannt.\\

\paragraph{Open MPI}\\

Open MPI\footnote{http://www.open-mpi.de/, abgerufen am 16.06.2015} ist die Implementation des \textit{Message Passing Interface}\footnote{http://www.mcs.anl.gov/research/projects/mpi/, abgerufen am 16.06.2015}, eines Standards zum Nachrichtenaustausch zwischen Prozessen.\\

Obwohl dieses Framework ebenfalls der Koordination unterschiedlicher Prozesse und einem gemeinsamen verteilten Speicherbereich dient, ist es völlig agnostisch bezüglich der ausgeführten Anwendung.\\
Sparks Domäne dagegen ist "`Large Scale Data Processing"', sämtliche Komponenten von Spark sind für diese Aufgaben optimiert. Überschneidungen der Anwendungsbereiche von Open MPI und Spark sind daher sehr unwahrscheinlich.\\