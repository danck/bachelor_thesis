\chapter{Vorstellung von Apache Spark}
Aus Sicht eines Nutzers ist Apache Spark eine API zum Zugriff auf Daten und deren Verarbeitung.\\

Diese API (wahlweise für die Sprachen Scala, Java und Python verfügbar), kann im einfachsten Fall über eine eigene Spark Konsole mit \gls{repl}\footcite{Hail} verwendet werden.\\
Die Zählung von Wortvorkommen in einem Text - das "`Hello World"' der Big Data Szene - lässt sich dort mit zwei Befehlen realisieren:\\

\begin{lstlisting}[caption=Word Count in der Spark Konsole]
my_dollar ./spark-shell
[...]
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.3.0
      /_/
Using Scala version 2.10.4 (OpenJDK 64-Bit Server VM, Java 1.7.0_75)
Type in expressions to have them evaluated.
[...]
scala> val text = sc.textFile("../Heinrich Heine - Der Ex-Lebendige")
[...]
scala> :paste
text.flatMap(line => line.split(" "))
.map(word => (word, 1))
.reduceByKey(_ + _)
.collect()
[...]
res0: Array[(String, Int)] = Array((Tyrann,,1), (im,2), (Doch,1) ...)

\end{lstlisting}


Aus Sicht eines Administrators ist Apache Spark eine Applikation auf einem Rechencluster, die sich in der Anwendungsschicht befindet und charakteristische Anforderungen insbesondere an Lokalität des Storages und die Netzwerkperformance stellt.\\

Was das konkret bedeutet, welche Mechanismen und Konzepte dahinterstehen und in welchem Ökosystem von Anwendungen sich Apache Spark bewegt wird im nächsten Abschnitt beleuchtet.

\section{Überblick}
\textcolor{gray}{--- Was ist es, wofür ist es, wie funktioniert es, (JVM, Scala) ---}

\begin{figure}[h]
\centering
  \begin{tikzpicture}
  \draw (7,0) node[RectObject, inner xsep=1cm] (Big2) {Big Node};
  \draw (6,2) node[RectObject] (Small2A) {A};
  \draw (8,2) node[RectObject] (Small2B) {B};
  \draw[arrow] (Small2A.north)--(Small2A|-Big2.south);
  \draw[arrow] (Small2B.south)--(Small2B|-Big2.north);
	\begin{umlpackage}{Apache Spark}
    \begin{umlcomponent}{A}
      \umlbasiccomponent{B}
      \umlbasiccomponent[y=-2]{C}
		\end{umlcomponent}
	\end{umlpackage}
  \end{tikzpicture}
  \caption{Verteilung der Software-Komponenten im Cluster}
\end{figure}
\section{Kernkonzepte}
\textcolor{gray}{--- Warum ist Spark schnell, ausfallsicher, secure (und wo vielleicht nicht)? ---}
\textcolor{gray}{--- Annahme: Durchsatzfaktoren RAM, Netzwerk, Festplatte ---}
\subsection{Nutzung von Arbeitsspeicher}
\subsection{Nutzung von persistentem Speicher}
\subsection{Nutzung von CPUs}
\subsection{Scheduling/Shuffling}
\subsection{Kern-API}

\section{Standardbibliotheken}
\textcolor{gray}{--- Warum ist Spark so einfach (und wo vielleicht nicht)? ---}\\
Die vier Standardbibliotheken erweitern die Kern-API für bestimmte, häufig genutzte Aufgaben aus Bereichen der Datenanalyse.\\

Die bedienten Bereiche sind
\begin{itemize}
	\item Deklaratives Abfragen und On-the-Fly-Transformationen (\textit{Spark SQL})
	\item Maschinenlernverfahren (\textit{MLlib})
	\item Echtzeitbehandlung von eingehenden Daten (\textit{Streaming})
	\item Operationen auf Graph-artigen Strukturen (\textit{GraphX})
\end{itemize}

\subsection{Dataframes/Spark SQL}
\subsection{MLlib}
\subsection{Streaming}
\subsection{GraphX}

\section{Betrieb und Security}

\section{Entwicklergemeinschaft}
\textcolor{gray}{--- Herkunft, Apache Foundation, Entwicklungsphilosophien, Anzahl Entwickler, ... ---}\\

\section{Verwandte Produkte}
\textcolor{gray}{--- Ergänzende oder konkurrierende Produkte ---}\\
\subsection{YARN}
\subsection{Mesos}
\subsection{Flink}
