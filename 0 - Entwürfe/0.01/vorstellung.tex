\chapter{Vorstellung von Apache Spark}
Aus Sicht eines Nutzers ist Apache Spark eine API zum Zugriff auf Daten und deren Verarbeitung.\\

Diese API (wahlweise für die Programmiersprachen Scala, Java und Python verfügbar), kann im einfachsten Fall über eine eigene Spark Konsole mit \gls{repl}\footcite{Hail} verwendet werden.\\
Die Zählung von Wortvorkommen in einem Text - das "`Hello World"' der Big Data Szene - lässt sich dort mit zwei Befehlen realisieren (Listing 2.1).\\

\begin{lstlisting}[caption=Word Count in der Spark Konsole]
my_dollar ./spark-shell
[...]
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.3.0
      /_/
Using Scala version 2.10.4 (OpenJDK 64-Bit Server VM, Java 1.7.0_75)
Type in expressions to have them evaluated.
[...]
scala> val text = sc.textFile("../Heinrich Heine - Der Ex-Lebendige")
[...]
scala> :paste
text.flatMap(line => line.split(" "))
.map(word => (word, 1))
.reduceByKey(_ + _)
.collect()
[...]
res0: Array[(String, Int)] = Array((Tyrann,,1), (im,2), (Doch,1) ...)

\end{lstlisting}


Aus Sicht eines Administrators oder Softwarearchitekten ist Apache Spark eine Applikation auf einem Rechencluster, die sich in der Anwendungsschicht befindet und charakteristische Anforderungen insbesondere an Lokalität des Storages und die Netzwerkperformance stellt.\\

Was das konkret bedeutet, welche Mechanismen und Konzepte dahinterstehen und in welchem Ökosystem von Anwendungen sich Apache Spark bewegt wird in den folgenden Abschnitten dieses Kapitels beleuchtet.

\section{Überblick}
\textcolor{gray}{--- Was ist es, wofür ist es, wie funktioniert es, (JVM, Scala) ---}

\begin{figure}[h]
\centering
  \begin{tikzpicture}
  \draw (7,0) node[RectObject, inner xsep=1cm] (Big2) {Big Node};
  \draw (6,2) node[RectObject] (Small2A) {A};
  \draw (8,2) node[RectObject] (Small2B) {B};
  \draw[arrow] (Small2A.north)--(Small2A|-Big2.south);
  \draw[arrow] (Small2B.south)--(Small2B|-Big2.north);
	\begin{umlpackage}{Apache Spark}
    \begin{umlcomponent}{A}
      \umlbasiccomponent{B}
      \umlbasiccomponent[y=-2]{C}
		\end{umlcomponent}
	\end{umlpackage}
  \end{tikzpicture}
  \caption{TODO: Verteilung der Software-Komponenten im Cluster}
\end{figure}

Um die Architektur und Optimierungskonzepte eines verteilten Systems bewerten zu können ist es offensichtlich wichtig, welche Eigenschaften der unterliegenden Hardware angenommen werden.

Weil Spark explizit für den Betrieb innerhalb eines Hadoop/YARN \textcolor{red}{[VERWEIS auf Abschnitt Scheduling]} geeignet ist und YARN wiederum für den Betrieb auf einem Rechnercluster auf Mittelklasse-Mehrzweckrechnern (Commodity Hardware) optimiert ist \textcolor{red}{[VERWEIS Apache Hadoop Yarn S. 42]}, kann für Spark von einer vergleichbaren Hardwarekonfiguration ausgegangen werden.\\

Der Vergleich von drei aktuellen Rack Servern der 2000-Euro-Klasse (in der Grundausstattung) \textcolor{red}{[VERWEIS Anhang]} - hier als Mittelklasse-Geräte bezeichnet - liefert die folgenden Verhältnisse der wesentlichen Schnittstellen.

\begin{table}[ht]
	\centering % used for centering table
	\begin{tabular}{c c c} % centered columns (4 columns)
		\hline\hline %inserts double horizontal lines
		Festspeicher & Netzwerk & Arbeitsspeicher\\ [0.5ex] % inserts table
		%heading
		\hline % inserts single horizontal line
		??? & 1,25 GB/s & 17GB/s \\ % inserting body of the table
		\hline %inserts single line
	\end{tabular}
	\caption{Vergleich von theoretischen Spitzenleistungen bei Mehrzweck-Servern der 2000 Euro Klasse} % title of Table
	\label{table:vgldurchsatz} % is used to refer this table in the text
\end{table}

Auf eine detaillierte Analyse des Zugriffsverhaltens wird im Rahmen dieser Arbeit verzichtet. Bei den folgenden Bewertungen der Kernkonzepte ist es wichtig sich die aus Tabelle \ref{table:vgldurchsatz} abgeleiteten Größenordnungen des Durchsatzes der verschiedenen Datenkanäle zu vergegenwärtigen:

\begin{equation*}
	Netzwerk < Festspeicher < Arbeitsspeicher
\end{equation*}

\section{Kernkonzepte}
\textcolor{gray}{--- Warum ist Spark schnell, ausfallsicher, secure (und wo vielleicht nicht)? ---}
\textcolor{gray}{--- Annahme: Durchsatzfaktoren RAM, Netzwerk, Festplatte ---}
\subsection{Nutzung von Arbeitsspeicher}
\subsection{Nutzung von persistentem Speicher}
\subsection{Nutzung von CPUs}
\subsection{Scheduling/Shuffling}
\subsection{Kern-API}

\section{Standardbibliotheken}
\textcolor{gray}{--- Warum ist Spark so einfach (und wo vielleicht nicht)? ---}\\
Die vier Standardbibliotheken erweitern die Kern-API für bestimmte, häufig genutzte Aufgaben aus Bereichen der Datenanalyse.\\

Die bedienten Bereiche sind
\begin{itemize}
	\item Deklaratives Abfragen und On-the-Fly-Transformationen (\textit{Spark SQL})
	\item Maschinenlernverfahren (\textit{MLlib})
	\item Echtzeitbehandlung von eingehenden Daten (\textit{Streaming})
	\item Operationen auf Graph-artigen Strukturen (\textit{GraphX})
\end{itemize}

\subsection{Dataframes/Spark SQL}
\subsection{MLlib}
\subsection{Streaming}
\subsection{GraphX}

\section{Betrieb und Security}

\section{Entwicklergemeinschaft}
\textcolor{gray}{--- Herkunft, Apache Foundation, Entwicklungsphilosophien, Anzahl Entwickler, ... ---}\\

\section{Verwandte Produkte}
\textcolor{gray}{--- Ergänzende oder konkurrierende Produkte ---}\\
\subsection{YARN}
\subsection{Mesos}
\subsection{Flink}
